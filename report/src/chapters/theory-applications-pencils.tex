\section{Theory and applications of pencils of matrices}
This chapter will introduce the reader to the concept of a linear pencil of matrices and its properties. Throughout this
chapter and the following one, a reader may refer to G\"antmacher \cite{gantmacher}, Kunkel, Mehrmann
\cite{kunkel-mehrmann} and Beelen, Van Dooren \cite{beelen-van_dooren}.
\begin{definition}[Linear pencil of matrices]
    A linear pencil of matrices is defined as a 1-degree polynomial with matrix coefficients
    \[
        \Gamma(\lambda) = A + \lambda B
    \]
    with \(\lambda \in \mathbb{C}, A\) and \(B\) \(m \times n\) matrices.
    A linear pencil of matrices may also be called a \textbf{pair of matrices} and, in this thesis, we shall use
    synonymously both terms.
\end{definition}

\subsection*{Regular pencils.}
Now, we consider the case where \((A + \lambda B)\) is a regular pencil of matrices.
\begin{definition}[Regular pencil]\cite[p. 25, Definition 2]{gantmacher}
    A matrix pair \((A, B)\) is said to be regular if and only if \(A\) and \(B\) are square matrices of the same size and
    the determinant \(det(A + \lambda B)\) is not identically zero.
\end{definition}

Consider the regular pencil of matrices
\[\Gamma(\lambda) = A + \lambda B,\]
let \(F\) be the field the entries of \(A\) and \(B\) belong to and \(r\) the rank of the pencil.

Denote with \(D_{j}(\lambda)\) the greatest common divisor of all minors of order \(j\) of \(\Gamma(\lambda)\)
(with \(j = 1, ..., r\)) and assume without any loss of generality \(D_{j}(\lambda)\) is monic and
\(D_{0}(\lambda) = 1\). Given the sequence,
\begin{gather*}
    D_{r}(\lambda), \
    D_{r-1}(\lambda), \
    ..., \
    D_{1}(\lambda), D_{0}(\lambda)
\end{gather*}
we define the \textbf{invariant polynomials} \cite[p. 26]{gantmacher} of the pencil of matrices
\(\Gamma(\lambda)\) as the fractions
\begin{gather*}
    i_{1}(\lambda) = \dfrac{D_{r}(\lambda)}{D_{r-1}(\lambda)}, \
    i_{2}(\lambda) = \dfrac{D_{r-1}(\lambda)}{D_{r-2}(\lambda)}, \
    ..., \
    i_{r}(\lambda) = D_{1}(\lambda).
\end{gather*}

We can now write the expansion of the invariant polynomials into irreducible factors in \(F\) as
\begin{gather*}
    i_{1}(\lambda) = \prod_{i=1}^{k}p_{i}(\lambda)^{\alpha_{1, i}}, \
    i_{2}(\lambda) = \prod_{i=1}^{k}p_{i}(\lambda)^{\alpha_{2, i}}, \
    ... \\
    i_{r}(\lambda) = \prod_{i=1}^{k}p_{i}(\lambda)^{\alpha_{r, i}},
\end{gather*}
with \(p_{i}\) an irreducible polynomial appearing in the expansion.

We define the \textbf{elementary} (\textbf{finite}) \textbf{divisors} \cite[p. 27]{gantmacher} \(e_{i}\) of
the pencil of matrices \(\Gamma(\lambda)\) all the polynomials \(p_{i}(\lambda)^{\alpha_{j, i}}\)
(with \(j = 1, ..., r\)) that are not equal to one.

A similar procedure may be defined for the pencil of matrices
\[
    \Theta(\mu, \lambda) = \mu A + \lambda B
\]
leading to polynomials in two variables \((\mu, \lambda)\). Clearly, having \(\mu = 1\) would lead to obtaining the elementary
finite divisors of \(\Gamma(\lambda)\); however, for each elementary divisor of degree \(q\) we have
\[
    e_{i}(\mu, \lambda) = \mu^q e_{i}(\dfrac{\lambda}{\mu}),
\]
and, with this technique, it is possible to generate all the elementary divisors of \(\Theta(\mu, \lambda)\) except for
those of the form \(\mu^q\), which are called \textbf{elementary infinite divisors} \cite[p. 27]{gantmacher} of
the pencil of matrices \(\Theta(\mu, \lambda)\).

\begin{remark} \cite[p. 27]{gantmacher}
    A regular pencil of matrices \(\Gamma(\lambda) = A + \lambda B\) has elementary infinite divisors if and only if
    \(det(B) = 0\).
\end{remark}

We can now give a result on the equivalence of regular pencils.

\begin{theorem}[Equivalence of regular pencils of matrices]\cite[p. 27, Theorem 2]{gantmacher} \label{thm:regular-pencils}
    Two regular matrix pairs
    \begin{align*}
        \Gamma_{1}(\lambda) &= (A_{1}, B_{1}) &
        \Gamma_{2}(\lambda) &= (A_{2}, B_{2})
    \end{align*}
    are equivalent if and only if they have the
    same finite and infinite elementary divisors.

    In other words, \(\Gamma_{1}\) and \(\Gamma_{2}\) are equivalent if and only if the invariant polynomials
    of the two pencils \(\Theta_{1}(\mu, \lambda)\) and \(\Theta_{2}(\mu, \lambda)\) are equivalent, with
    \begin{align*}
        \Theta_{1}(\mu, \lambda) &= \mu A + \lambda B &
        \Theta_{2}(\mu, \lambda) &= \mu A + \lambda B
    \end{align*}
\end{theorem}

\begin{lemma} \cite[Equation 2.14]{ikramov}
    For any two regular equivalent pencils of \(n \times n\) matrices the entries of which lie in a field \(F\)
    \begin{align*}
        \Gamma_{1}(\lambda) &= A_{1} + \lambda B_{1} &
        \Gamma_{2}(\lambda) &= A_{2} + \lambda B_{2}
    \end{align*}
    there exist two nonsingular matrices \(P \in F^{n \times n}\), \(Q \in F^{n \times n}\) such that
    \[
        P \Gamma_{1}(\lambda) Q = \Gamma_{2}(\lambda),
    \]
    and we call such a transformation an \textbf{equivalence transformation}.
\end{lemma}

\subsection*{Singular pencils.}
Next, we shall investigate the most general case of \(m \times n\) pencils of matrices in order to introduce
the reader to the concept of minimal indices of a pencil of matrices.
\begin{definition}[Singular pencil]\cite[p. 25, Definition 2]{gantmacher}
    A matrix pair \(A, B\) is said to be singular if and only if it is not regular.
\end{definition}

Consider the singular pencil of (rectangular) matrices \(\Gamma(\lambda) = A + \lambda B\) and assume its rank \(r\) is smaller
than its number of columns \(n\).

This implies the equation
\[
    (A + \lambda B) \vb{x} = 0
\]
has nontrivial solutions \(\vb{x}_{1}(\lambda), ..., \vb{x}_{k}(\lambda)\) and let \(X\) be the polynomial
matrix made up of such polynomials
\[
    X =
    \begin{bmatrix}
        x_{1, 1} & x_{2, 1} & \ldots & x_{k, 1} \\
        x_{1, 2} & x_{2, 2} & \ldots & x_{k, 1} \\
        \vdots   &          &        &   \vdots \\
        x_{1, n} & x_{2, n} & \ldots & x_{k, n}
    \end{bmatrix}
\]

The columns \(\vb{x}_{i}(\lambda)\) (with \(i = 1, ..., k\)) of \(X\) can be chosen to be linearly independent; as a matter of
fact, the columns are linearly dependent if the rank of \(X\) is less than \(k\) and, (only) in this case, we can
choose \(k\) nontrivial polynomials \(p_{i}(\lambda)\) such that
\[
    \sum_{i=1}^{k} p_{i}(\lambda) \vb{x}_{i}(\lambda) \equiv 0.
\]

We choose the nontrivial polynomial \(\vb{x}_{1}(\lambda)\) of least degree \(\epsilon_{1}\) in \(\lambda\); next,
amongst the very same solutions, we choose \(\vb{x}_{2}(\lambda)\) of least degree \(\epsilon_{2}\) etc.

Since the maximum number of linearly independent solutions of the aforementioned equation
is no more than \(n - r\), this process has a finite number of steps.

To summarize, from an equation \((A + \lambda B)\vb{x} = 0\) we can obtain a sequence of solutions of non-increasing degree
\[
    \epsilon_{1} \leq \epsilon_{2} \leq ... \leq \epsilon_{p},
\]
and we define \(\epsilon_{i}\) a \textbf{minimal index for the columns} \cite[p. 38]{gantmacher} of the pencil of matrices
\(\Gamma(\lambda)\).

We can also introduce \textbf{minimal indices for the rows} \cite[p. 38]{gantmacher} \(\eta_{1}, \eta_{2}, ..., \eta_{q}\)
of the pencil \((A + \lambda B)\) which we can yield working with the transpose of the pencil \(A^T + \lambda B^T\)
or, in other words, with the equation
\[
    (A^T + \lambda B^T)\vb{y} = 0.
\]

We can now give another result on the equivalence of pencils of matrices.

\begin{theorem}[Necessary condition for the equivalence of pencils] \cite[p. 39]{gantmacher} \label{thm:singular-pencils}
    Two arbitrary equivalent pencils have the same minimal indices for rows and columns.
\end{theorem}

\subsection*{Kronecker canonical form.}
At this point, we can put together the theorems on the equivalence of regular and singular pencil of matrices, namely
\nameref{thm:regular-pencils} (theorem \ref{thm:regular-pencils}) and
\nameref{thm:singular-pencils} (theorem \ref{thm:singular-pencils}) and give a result on the \textbf{equivalence of
arbitrary matrix pairs}.
\begin{theorem}[Kronecker] \cite[p. 40, Theorem 5]{gantmacher} \label{thm:kronecker}
    Two pencils \((A + \lambda B)\), \((A_{1} + \lambda B_{1})\) of rectangular \(m \times n\) matrices
    are equivalent if and only if they have the same minimal indices for rows and columns and the same elementary finite and
    infinite divisors.
\end{theorem}

\begin{remark}
    Restating \nameref{thm:kronecker} (theorem \ref{thm:kronecker}) using different words, we can say that a matrix pair
    \((A, B)\) is completely characterized by its minimal indices for rows and columns and (its) elementary finite and
    infinite divisors and does not depend on their order.
\end{remark}

Coherently, it should be possible to define a canonical form for pencils of matrices completely determined by both
the minimal indices for rows and columns and the elementary finite and infinite divisors, and it is. We can now introduce
the Kronecker canonical form of a matrix pair.

\begin{theorem}[Kronecker canonical form]\label{thm:kcf}
    Let \(\Gamma(\lambda) = A + \lambda B\) be an arbitrary pencil of matrices and \(h\), \(g\) the maximal number of constant
    independent solutions of the two equations
    \begin{align*}
        (A + \lambda B)\vb{x} &= 0 & (A^T + \lambda B^T)\vb{y} &= 0.
    \end{align*}
    The pencil \(\Gamma(\lambda)\) is strictly equivalent to a quasi-diagonal matrix
    \[
        \begin{bmatrix}
            O^{(h, g)} \\
            & L \\
            & & L^T \\
            & & & N \\
            & & & & G + \lambda I
        \end{bmatrix}
    \]
    with \(G\) a Jordan matrix, and the (other) blocks defined as
    \begin{align*}
        L &= \begin{bmatrix}
            L_{\epsilon_{h+1}} \\
            & L_{\epsilon_{h+2}} \\
            & & \ddots \\
            & & & L_{\epsilon_{p}}
        \end{bmatrix}, &
        L^T &= \begin{bmatrix}
            L_{\eta_{h+1}}^T \\
            & L_{\eta_{h+2}}^T \\
            & & \ddots \\
            & & & L_{\eta_{q}^T}
        \end{bmatrix}, &\\
        N &= \begin{bmatrix}
            N^{(u_{1})} \\
            & N^{(u_{2})} \\ 
            & & \ddots \\
            & & & N^{(u_{s})}
        \end{bmatrix}, &
        L_{i} &=
        \begin{bmatrix}
            \lambda        &      1     &       0      &     \ldots       &    0       &    0   \\
            0              & \lambda    &       1      &                  & \vdots     & \vdots \\
            \vdots         & \vdots     &     & \ddots    &      &            &        \\
            0              &      0     &              &                  & \lambda    &    1   
        \end{bmatrix};
    \end{align*}
    \(L_{i}\) is a rectangular \(i \times (i + 1)\) matrix and
    \[
        N^{(u)} = I^{(u)} + \lambda H^{(u)}.
    \]

    We shall call such a matrix the Kronecker canonical form of \(\Gamma(\lambda)\) and write it
    out as \(K(\Gamma(\lambda))\).
\end{theorem}

For a proof of \nameref{thm:kronecker} (theorem \ref{thm:kronecker}) and \nameref{thm:kcf} (theorem \ref{thm:kcf})
the reader shall refer to the following chapter.

\begin{corollary}
    Given a matrix pair \((A, B)\) there exists a pair of nonsingular matrices \(P\), \(Q\) such that
    \begin{align*}
        P A Q &= K(A) & P B Q &= K(B),
    \end{align*}
    with P an \(m \times m\) and Q an \(n \times n\) matrices respectively.
\end{corollary}

\begin{example}
    Kronecker's canonical form of the pencil of matrices
    \[
        \begin{bNiceArray}{>{\strut}llllll}[margin=3mm]
            \Block[draw=red]{1-1}{}0 \\
            & \Block[draw=red]{2-1}{} \lambda \\
            &                            1    \\
            & & \Block[draw=red]{1-1}{} 1 \\
            & & & \Block[draw=red]{2-2}{} 1 & \lambda \\
            & & &                         0 & 1 \\
            & & & & & \Block[draw=red]{1-1}{} \lambda + 1
        \end{bNiceArray}.
    \]
    is made up of three blocks \(O^{(1, 1)}\), \(L_{0}\), \(L_{1}^T\), two nilpotent blocks
    \(N^{(1)}\), \(N^{(2)}\) and one Jordan block \(G + \lambda I^{(1)}\).
\end{example}

\subsection*{Fundamental applications.}
The following section shall introduce the reader to differential-algebraic equations (which we may shorten as DAE) in
order to explain what is the use case of the KCF of a pencil of matrices while keeping a strong focus on examples.

\begin{definition}[Differential-algebraic equation] \cite[p. 3, Equation 1.1]{kunkel-mehrmann}\label{def:dae}
    An equation
    \[
        F(t, x, \dot{x}) = 0
    \]
    with \(F: I \times D_{x} \times D_{x} \rightarrow \mathbb{C}^m\), \(I \subseteq \mathbb{R}\) is a compact interval,
    \(D_{x}\), \(D_{x} \subseteq \mathbb{C}^n\) are open, \(x: I \rightarrow \mathbb{C}^n\) a differentiable function
    and \(m\), \(n \in \mathbb{N}\) is called differential-algebraic equation.

    What we're attempting to determine is a function \(x\) such that
    \[
        \forall x \in I. F(t, x(t), \dot{x(t)}) = 0.
    \]
\end{definition}

Differential-algebraic equations are used to model physical systems with a dynamic behaviour the states of which are
subjected to certain constraints. For our purposes, we'll narrow the definition given above and rewrite it as follows.
\begin{definition}[Linear DAE with constant coefficients] \cite[p. 13, Equation 2.1]{kunkel-mehrmann}
    An equation
    \[
        B \dot{x} + Ax = f(t)
    \]
    with (\(A\), \(B \in \mathbb{C}^{m \times n}\)),
    \(f: I \rightarrow \mathbb{C}^m\) a continuous and twice-differentiable function, \(I \subseteq \mathbb{R}\) a
    compact interval and \(m \in \mathbb{N}\) is called a linear differential-algebraic equation with constant
    coefficients.

    It may be paired with an initial condition
    \[
        x(t_{0}) = x_{0}.
    \]
\end{definition}

\begin{remark}\cite[p. 13]{kunkel-mehrmann}
    The equations \(B \dot{x} + Ax = f(t)\) and \(\tilde{B} \tilde{\dot{x}} + \tilde{A}\tilde{x} = \tilde{f}(t)\), with
    \begin{align*}
        \tilde{A} &= PAQ, & \tilde{B} &= PBQ, & \tilde{f} &= Pf, & \tilde{x} &= Q^{-1}x, \\
        && P \in \mathbb{C}^{m \times m}, && Q \in \mathbb{C}^{n \times n}
    \end{align*}
    and \(P\), \(Q\) nonsingular matrices, are both linear differential-algebraic equations with constant coefficients and
    the relation \(x = Q\tilde{x}\) provides a one-to-one mapping between their solution sets.
\end{remark}

Next, we shall take as examples pencils of matrices made up of the blocks described before in order to show how the
linear DAE with constant coefficients associated may be solved.
\pagebreak
\begin{example} \label{exmp:dae-l2}
    Consider the pencil
    \[
        \Gamma(\lambda) =
        \begin{bmatrix}
            \lambda & 1 & 0 \\
            0 & \lambda & 1
        \end{bmatrix}.
    \]
    We write the linear DAE with constant coefficients associated with \(\Gamma(\lambda)\) as
    \[
        \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0
        \end{bmatrix}
        \begin{bmatrix}
            \dot{x_{1}} \\
            \dot{x_{2}} \\
            \dot{x_{3}}
        \end{bmatrix}
        + \begin{bmatrix}
            0 & 1 & 0 \\
            0 & 0 & 1
        \end{bmatrix}
        \begin{bmatrix}
            x_{1} \\
            x_{2} \\
            x_{3}
        \end{bmatrix} =
        \begin{bmatrix}
            f_{1}(t) \\
            f_{2}(t)
        \end{bmatrix}.
    \]

    To compute its solutions we must solve the linear system associated with the DAE specified above
    \begin{equation}
        \left\{
        \begin{aligned}
            \dot{x_{1}} + x_{2} - f_{1}(t) & = 0 \\
            \dot{x_{2}} + x_{3} - f_{2}(t) & = 0
        \end{aligned}
        \right.
    \end{equation}
    Denoting with \(g\) a generic \(C^2\) function, the solution is
    \begin{equation*}
        \left\{
            \begin{aligned}
                x_{1} &= g \\
                x_{2} &= -\dot{x_{1}} + f_{1}(t) \\
                x_{3} &= \ddot{x_{1}} - \dot{f_{1}}(t) + f_{2}(t)
            \end{aligned}
        \right.
    \end{equation*}
\end{example}
\pagebreak
\begin{example} \label{exmp:dae-l2T}
    For this example, we'll take the transpose of the pencil of matrices given in example \ref{exmp:dae-l2}
    \[
        \Gamma(\lambda) =
        \begin{bmatrix}
            \lambda & 0 \\
            1 & \lambda \\
            0 & 1
        \end{bmatrix}.
    \]
    The DAE associated with \(\Gamma(\lambda)\) is
    \[
        \begin{bmatrix}
            1 & 0 \\
            0 & 1 \\
            0 & 0
        \end{bmatrix}
        \begin{bmatrix}
            \dot{x_{1}} \\
            \dot{x_{2}}
        \end{bmatrix} + 
        \begin{bmatrix}
            0 & 0 \\
            1 & 0 \\
            0 & 1
        \end{bmatrix}
        \begin{bmatrix}
            x_{1} \\
            x_{2}
        \end{bmatrix}=
        \begin{bmatrix}
            f_{1}(t) \\
            f_{2}(t) \\
            f_{3}(t)
        \end{bmatrix}.
    \]

    We write this as a linear system
    \begin{equation*}
        \left\{
            \begin{aligned}
                & \dot{x_{1}} - f_{1}(t) = 0 \\
                & \dot{x_{2}} + x_{1} - f_{2}(t) = 0 \\
                & x_{2} - f_{3}(t) = 0
            \end{aligned}
        \right.
    \end{equation*}

    The system can be solved if and only if
    \[
        f_{1}(t) = \dot{f_{2}}(t) - \ddot{f_{3}}(t),
    \]
    and its solution is
    \begin{equation*}
        \left\{
            \begin{aligned}
                & x_{1} = f_{2}(t) - \dot{f_{3}}(t) \\
                & x_{2} = f_{3}(t)
            \end{aligned}
        \right.
    \end{equation*}
\end{example}
\pagebreak
\begin{example}
    Let \(\Gamma(\lambda)\) be the pencil of matrices
    \[
        \Gamma(\lambda) =
        \begin{bmatrix}
            1 & \lambda & 0 & 0 \\
            0 & 1 & \lambda & 0 \\
            0 & 0 & 1 & \lambda \\
            0 & 0 & 0 & 1
        \end{bmatrix}.
    \]
    Rewriting this as a linear DAE with constant coefficients yields
    \[
        \begin{bmatrix}
            0 & 1 & 0 & 0 \\
            0 & 0 & 1 & 0 \\
            0 & 0 & 0 & 1 \\
            0 & 0 & 0 & 0
        \end{bmatrix}
        \begin{bmatrix}
            \dot{x_{1}} \\
            \dot{x_{2}} \\
            \dot{x_{3}} \\
            \dot{x_{4}}
        \end{bmatrix} +
        \begin{bmatrix}
            1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 1 & 0 \\
            0 & 0 & 0 & 1
        \end{bmatrix}
        \begin{bmatrix}
            x_{1} \\
            x_{2} \\
            x_{3} \\
            x_{4}
        \end{bmatrix}
        =
        \begin{bmatrix}
            f_{1}(t) \\
            f_{2}(t) \\
            f_{3}(t) \\
            f_{4}(t)
        \end{bmatrix}
    \]
    We follow the same steps as in other examples
    \begin{equation*}
        \left\{
            \begin{aligned}
                & \dot{x_{2}} + x_{1} - f_{1}(t) = 0 \\
                & \dot{x_{3}} + x_{2} - f_{2}(t) = 0 \\
                & \dot{x_{4}} + x_{3} - f_{3}(t) = 0 \\
                & x_{4} - f_{4}(t) = 0
            \end{aligned}
        \right.
    \end{equation*}

    The solution is
    \begin{equation*}
        \left\{
            \begin{aligned}
                & x_{1} = f_{1}(t) - \dot{f_{2}}(t) + \ddot{f_{3}}(t) - \dddot{f_{4}}(t) \\
                & x_{2} = f_{2}(t) - \dot{f_{3}}(t) + \ddot{f_{4}}(t) \\
                & x_{3} = f_{3}(t) - \dot{f_{4}}(t) \\
                & x_{4} = f_{4}(t)
            \end{aligned}
        \right.
    \end{equation*}
\end{example}
\pagebreak
\begin{example}
    Let us take the pencil
    \[
        \Gamma(\lambda) =
        \begin{bmatrix}
            \lambda & 1 & 0 & 0 \\
            0 & \lambda & 0 & 0 \\
            0 & 0 & \lambda + 2 & 1 \\
            0 & 0 & 0 & \lambda + 2
        \end{bmatrix}
    \]
    and write the DAE associated with it
    \[
        \begin{bmatrix}
            1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 1 & 0 \\
            0 & 0 & 0 & 1
        \end{bmatrix}
        \begin{bmatrix}
            \dot{x_{1}} \\
            \dot{x_{2}} \\
            \dot{x_{3}} \\
            \dot{x_{4}}
        \end{bmatrix}
        + \begin{bmatrix}
            0 & 1 & 0 & 0 \\
            0 & 0 & 0 & 0 \\
            0 & 0 & 2 & 1 \\
            0 & 0 & 0 & 2
        \end{bmatrix}
        \begin{bmatrix}
            x_{1} \\
            x_{2} \\
            x_{3} \\
            x_{4}
        \end{bmatrix}
        = \begin{bmatrix}
            f_{1}(t) \\
            f_{2}(t) \\
            f_{3}(t) \\
            f_{4}(t)
        \end{bmatrix}.
    \]

    The linear system to solve is
    \begin{equation*}
        \left\{
            \begin{aligned}
                & \dot{x_{1}} + x_{2} - f_{1}(t) = 0 \\
                & \dot{x_{2}} - f_{2}(t) = 0 \\
                & \dot{x_{3}} + 2x_{3} + x_{4} - f_{3}(t) = 0 \\
                & \dot{x_{4}} + 2x_{4} - f_{4}(t) = 0
            \end{aligned}
        \right.
    \end{equation*}
    Denoting with \(g\) a \(C^2\) function such that
    \[
        \ddot{g} = \dot{f_{1}}(t) - f_{2}(t),
    \]
    and (with) \(D\), \(K\) arbitrary constants, the solution is
    \begin{equation*}
        \left\{
            \begin{aligned}
                & x_{1} = g \\
                & x_{2} = f_{1}(t) - \dot{g} \\
                & x_{3} = -\frac{1}{4} \, {\left(4Kt - 2f_{3}(t) e^{2t} + f_{4}(t)e^{2t} - 4K\right)} e^{-2t}\\
                & x_{4} = \frac{1}{2} \, {\left(f_{4}(t) e^{2t} + 2D\right)} e^{-2t}\\
            \end{aligned}
        \right.
    \end{equation*}
\end{example}