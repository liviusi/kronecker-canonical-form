\section{Computation of Kronecker's Canonical Form}
The following chapter deals with the problem of computing Kronecker's canonical form for a matrix pair \((A, B)\).
The approach described will be shown to be correct, and an implementation
of it using the CAS SageMath has been made publicly available under MIT License on GitHub
\cite{Trapani_Computation_of_Kronecker_s}.


We shall divide it into two subproblems: the first one deals with regular pencils of matrices, the other with
singular pencils.

At the end of each of the following sections, the steps described shall be summarised in pseudocode.

\subsection*{Regular pencils.}
Let \(\Gamma(\lambda) = A + \lambda B\) be a regular pencil of matrices defined in a vector space over a
field \(F\).

First, we shall find a value \(c \in F\) such that \(det(\Gamma(c)) \neq 0\) and define the matrix 
\[
    A_{1} = A + cB.
\]

Now, we rewrite \(\Gamma(\lambda)\) in terms of \(A_{1}\),
\[
    \Gamma(\lambda) = A_{1} + (\lambda - c)B,
\]
and premultiply it by \(A_{1}^{-1}\)
\begin{gather}
    A_{1}^{-1} \Gamma(\lambda) = I + (\lambda - c)A_{1}^{-1}B. \label{gamma-reg-1}
\end{gather}

Let us denote with \(J\) the JCF of \(A_{1}^{-1}B\) and \(P_{1}\) the similarity matrix used to compute it
\[
    J = P_{1}^{-1}(A_{1}^{-1}B)P_{1},
\]
and let us assume it is of the form
\[
    J =
    \begin{bmatrix}
        J_{1} & \\
        & J_{0}
    \end{bmatrix}
\]
with \(J_{0}\) a nilpotent Jordan matrix and \(J_{1}\) such that \(det(J_{1}) \neq 0\) (which is the form returned by
SageMath when computing Jordan matrices); call \(j\), \(k\) the number of rows in \(J_{0}\) and \(J_{1}\) respectively.

Define the permutation matrix
\[
    P_{\pi} =
    \begin{bmatrix}
        & I^{(k)}\\
        I^{(j)} &
    \end{bmatrix}
\]
so that we can write
\[
    P_{\pi}^T J P_{\pi} = J' =
    \begin{bmatrix}
        J_{0} & \\
        & J_{1}
    \end{bmatrix}.
\]

Now, we rewrite \eqref{gamma-reg-1} substituting \(A_{1}^{-1}B\) with its permuted Jordan form
\begin{gather}
    A_{1}^{-1} \Gamma(\lambda) = I + (\lambda - c)P_{1} P_{\pi}^{-T} J' P_{\pi}^{-1} P_{1}^{-1}.
    \label{gamma-reg-1.5}
\end{gather}
We can write the identity matrix in terms of \(P_{1}\) as \(I = P_{1}P_{1}^{-1}\) in \eqref{gamma-reg-1.5}
\begin{align*}
    & A_{1}^{-1} \Gamma(\lambda) = \\
    &= P_{1}P_{1}^{-1} + (\lambda - c)P_{1} P_{\pi}^{-T} J' P_{\pi}^{-1} P_{1}^{-1} = \\
    &= P_{1}(P_{1}^{-1} + (\lambda - c)P_{\pi}^{-T} J' P_{\pi}^{-1} P_{1}^{-1}).
\end{align*}
We premultiply it by \(P_{1}^{-1}\)
\[
    P_{1}^{-1} A_{1}^{-1} \Gamma(\lambda) = P_{1}^{-1} + (\lambda - c)P_{\pi}^{-T} J' P_{\pi}^{-1} P_{1}^{-1}
\]
and then postmultiply by \(P_{1}\)
\[
    P_{1}^{-1} A_{1}^{-1} \Gamma(\lambda) P_{1} = I + (\lambda - c)P_{\pi}^{-T} J' P_{\pi}^{-1}.
\]

The very same steps can be followed for \(P_{\pi}^{-T}\) and \(P_{\pi}^{-1}\).

The final result is
\begin{gather}
    P_{\pi}^T P_{1}^{-1} A_{1}^{-1} \Gamma(\lambda) P_{1} P_{\pi} = I + (\lambda - c)J'. \label{gamma-reg-2}
\end{gather}

Now, we may work on the blocks \(J_{0}\), \(J_{1}\) of \(J'\).

For the following steps, it is useful to rewrite the expression on the right side of \eqref{gamma-reg-2} so that
the form of its diagonal blocks is explicitly readable, meaning
\[
    I + (\lambda - c)J' =
    \begin{bmatrix}
        I^{(j)} - cJ_{0} + \lambda J_{0} & \\
        & I^{(k)} - cJ_{1} + \lambda J_{1}
    \end{bmatrix}.
\]

Let us start from the first diagonal block
\[
    I^{(j)} - cJ_{0} + \lambda J_{0}.
\]
First, we need to postmultiply it in
\eqref{gamma-reg-2} by \(K = (I^{(j)} - cJ_{0})^{-1}\)
\begin{gather*}
    \scalemath{0.92}{
        P_{\pi}^T P_{1}^{-1} A_{1}^{-1} \Gamma(\lambda) P_{1} P_{\pi}
        \begin{bmatrix}
            K& \\
            & I^{(k)}
        \end{bmatrix}
        = 
        \begin{bmatrix}
            I^{(j)} + \lambda K J_{0} & \\
            & I^{(k)} - cJ_{1} + \lambda J_{1}
        \end{bmatrix}.
    }
\end{gather*}

Let us denote with \(H^{(j)}\) the JCF of \(K J_{0}\), \(P_{2}\) the similarity matrix
used to compute it and \(N^{(j)} = I^{(j)} + \lambda H^{(j)}\)
\[
    N^{(j)} = I^{(j)} + \lambda H^{(j)} = I^{(j)} + \lambda P_{2}^{-1} (K J_{0}) P_{2}.
\]

\begin{remark}
    H is an upper shift matrix as it is the JCF of a nilpotent matrix.
\end{remark}

Following the analogous steps to handle \(P_{2}\) and its inverse yields us
\begin{gather}
    \scalemath{0.92}{
        \begin{bmatrix}
            P_{2}^{-1} & \\
            & I
        \end{bmatrix}
        P_{\pi}^T P_{1}^{-1} A_{1}^{-1} \Gamma(\lambda) P_{1} P_{\pi}
        \begin{bmatrix}
            KP_{2} & \\
            & I^{(k)}
        \end{bmatrix} =
        \begin{bmatrix}
            N^{(j)} & \\
            & I^{(k)} - cJ_{1} + \lambda J_{1} 
        \end{bmatrix}. \label{gamma-reg-3}
    }
\end{gather}

At this point, we can focus on the second diagonal block
\[
    I^{(k)} - cJ_{1} + \lambda J_{1}.
\]
We postmultiply it in
\eqref{gamma-reg-3} by \(J_{1}^{-1}\)
\begin{gather}
    \scalemath{0.92}{
        \begin{bmatrix}
            P_{2}^{-1} & \\
            & I
        \end{bmatrix}
        P_{\pi}^T P_{1}^{-1} A_{1}^{-1} \Gamma(\lambda) P_{1} P_{\pi}
        \begin{bmatrix}
            KP_{2} & \\
            & J_{1}^{-1}
        \end{bmatrix} =
        \begin{bmatrix}
            N^{(j)} & \\
            & J_{1}^{-1} + (\lambda - c)I^{(k)} 
        \end{bmatrix}.
    }
\end{gather}

Let us denote with \(G\) the JCF of the constant term \(J_{1}^{-1} - cI^{(k)}\) and \(P_{3}\) the similarity matrix
used to compute it
\[
    J_{1}^{-1} - cI^{(k)} = P_{3}^{-1}GP_{3}.
\]

Again, we follow the same procedure to handle \(P_{3}\) and its inverse, thus obtaining our end result
\begin{gather}
    \scalemath{0.975}{
        \begin{bmatrix}
            P_{2}^{-1} & \\
            & P_{3}^{-1}
        \end{bmatrix}
        P_{\pi}^T P_{1}^{-1} A_{1}^{-1} \Gamma(\lambda) P_{1} P_{\pi}
        \begin{bmatrix}
            KP_{2} & \\
            & J_{1}^{-1}P_{3}
        \end{bmatrix} =
        \begin{bmatrix}
            N^{(j)} & \\
            & G + \lambda I^{(k)}
        \end{bmatrix}. \label{gamma-reg}
    }
\end{gather}

To conclude, we shall present the aforementioned steps used in order to compute Kronecker's canonical form
\(K(\Gamma(\lambda))\) in pseudocode.
\begin{algorithm}
    \caption{Procedure to compute KCF of a regular pencil.}\label{alg:kcf-regular}
    \KwData{$\Gamma(\lambda) = A + \lambda B$: regular pencil}
    \KwResult{$K$: KCF of the pencil of matrices $\Gamma(\lambda)$}
    \While{True}{
        $c \gets $ random value\;
        \If{$det(\Gamma(c)) \neq 0$}{
            \Break
        }
    }
    $A_{1} \gets A + c*B$\;
    $J \gets $ jordan($A_{1}^{-1}*B$)\;
    $\{J_{0}, J_{1}\} \gets$ submatrices($J$)\;
    $\{N_{i}\}_{i \geq 0} \gets $ jordan($(I - c*J_{0})^{-1} * J_{0}$)\;
    $G \gets $ jordan($J_{1}^{-1} - c*I$)\;
    $K \gets $ diag($\{N_{i}\}, G +\lambda I$)\;
    \Return{K}\;
\end{algorithm}

We have now proved the following theorem.
\begin{theorem}[KCF of a regular pencil of matrices]
    Every regular pencil of matrices \(\Gamma(\lambda) = A + \lambda B\) can be reduced to a matrix of the form
    \[
        \begin{bmatrix}
            N^{(u_{1})} \\
            & N^{(u_{2})} \\
            & & \ddots \\
            & & & N^{(u_{s})} \\
            & & & & G + \lambda I
        \end{bmatrix},
    \]
    where the first \(s\) diagonal blocks correspond to infinite elementary divisors
    \(\mu^{u_{1}}, ..., \mu^{u_{s}}\) of \(\Gamma(\lambda)\) and the last block is uniquely determined by the
    finite elementary divisors of the given pencil.
\end{theorem}

\begin{theorem}[Stability of KCF transformation for a regular pencil] \label{thm:stability-kcf-regular}
    The problem of computing Kronecker's canonical form for a regular pencil
    is ill-conditioned.
\end{theorem} 

We can see \nameref{thm:stability-kcf-regular} (theorem \ref{thm:stability-kcf-regular}) is
trivially true as it involves the computation of many JCFs.

\subsection*{Singular pencils.}

Let \(\Gamma(\lambda)\) be a singular pencil of \(m \times n\) matrices the entries of which lie in a field \(F\),
and assume without loss of generality the relation \(r < n\) holds, with \(r\) the rank of the pencil
\[\Gamma(\lambda) = A + \lambda B.\]

First, we shall find the polynomial \(x(\lambda)\) of minimal degree \(\epsilon\) in the (right) kernel
of \(\Gamma(\lambda)\)
\[
    x(\lambda) = x_{0} - \lambda x_{1} + \lambda^2 x_{2} + ... + (-1)^\epsilon \lambda^\epsilon x_{\epsilon}.
\]
In order to define a procedure to compute it, we consider the family of matrices
\(M_{k}\) defined as
\begin{align} \label{m-matrices}
    M_{0} &=
        \begin{bmatrix}
            A \\
            B
        \end{bmatrix},
    & M_{1} &=
        \begin{bmatrix}
            A & 0 \\
            B & A \\
            0 & B
        \end{bmatrix},
    & M_{k-1} &=
        \begin{bmatrix}
            A & 0 & \hdots &    0   \\
            B & A &        & \vdots \\
            0 & B & \ddots & \\
            \vdots & \vdots & \ddots & A \\
            0      &    0   & \hdots & B
        \end{bmatrix}.
\end{align}
The (right) kernel of a matrix \(M_{k}\) is
\begin{align*}
   Ax_{0} &= 0, &
   Bx_{0} - Ax_{1} &= 0, &
   & ..., &
   Bx_{k-1} - Ax_{k} &= 0, &
   Bx_{k} &= 0,
\end{align*}
and its basis matrix is the vector associated with the polynomial \(g(\lambda)\) of degree \(k\). Note that if
there is no polynomial of degree \(k\) in the kernel of the pencil \(\Gamma(\lambda)\), then \(ker(M_{k})\) is empty.

Iteratively building the succession of matrices \(\{M_{i}\}_{i \geq 0}\) and computing their right kernels
yields us the polynomial
\(x(\lambda)\), which is what we were looking for.

We know by construction both the vectors in
\(
    B_{1} = \{x_{0}, x_{1}, ..., x_{\epsilon}\}
\)
and those in
\(
    B_{2} = \{Ax_{0}, Ax_{1}, ..., Ax_{\epsilon}\}
\)
are linearly independent.

Denote with \(P, Q\) the matrices obtained by completing the vectors in \(B_{2}, B_{1}\) to a basis in; applying a
change of basis to the pencil of matrices \(\Gamma(\lambda)\) gives us
\[
    \tilde{\Gamma}(\lambda) = \tilde{A} + \lambda \tilde{B} =  P^{-1}AQ + \lambda P^{-1}BQ,
\]
with \(\tilde{A}\), \(\tilde{B}\) of the kind
\[
    \tilde{A} =
        \begin{bNiceArray}{>{\strut}llllllll}[margin=3mm]
            \Block[draw=red]{1-5}{}
            0 & 1 & & \hdots & 0 & * & \hdots & * \\
            0 & 0 & 1 & \hdots & 0 & * & \hdots & * \\
            \vdots & \vdots & & \ddots & \vdots & \hdotsfor{3} \\
            0 & 0 & & & 1 & * & \hdots & *\\
            0 & 0 & & & 0 & * & \hdots & * \\
            \hdots & \hdots & \hdots & \hdots & \hdots & \hdots & \hdots & \hdots \\
            0 & 0 & & \hdots & 0 & * & \hdots & *
        \end{bNiceArray},
\]
\[
    \tilde{B} =
        \begin{bNiceArray}{>{\strut}llllllll}[margin=3mm]
            \Block[draw=red]{1-5}{}
            1 & 0 & \hdots & 0 & 0 & * & \hdots & * \\
            0 & 1 & \hdots & 0 & 0 & * & \hdots & * \\
            \vdots & & \ddots & & \vdots & \hdotsfor{3} \\ 
            0 & 0 & \hdots & 1  & 0 & * & \hdots & *\\
            0 & 0 & & & 0 & * & \hdots & *\\
            0 & 0 & & & 0 & * & \hdots & * \\
            \hdots & \hdots & \hdots & \hdots & \hdots & \hdots & \hdots & \hdots \\
            0 & 0 & & \hdots & 0 & * & \hdots & *
        \end{bNiceArray};
\]
the coloured block spans over the first \(\epsilon + 1\) columns, the notation \(* \hdots *\) is used to
denote a block with no relevant structure.

For the sake of clarity, we shall partition the pencil of matrices \(\tilde{\Gamma}(\lambda)\) as
\[
    \begin{bmatrix}
        L_{\epsilon} & D + \lambda F \\
        0 & A^{*} + \lambda B^{*}
    \end{bmatrix},
\]
with \(L_{\epsilon}\) the first rectangular \(\epsilon \times (\epsilon + 1)\) block.

Next, we distinguish two different cases depending on the value of the degree \(\epsilon\).
\begin{cs}
    \case \(\epsilon > 0.\)
    We now need to transform the pencil of matrices \(\tilde{\Gamma}(\lambda)\) so that it is upper triangular;
    in other words, we shall determine two matrices \(X, Y\) such that
    \[
        D + \lambda F + Y(A^{*} + \lambda B^{*}) - L_{\epsilon}X = 0
    \]
    Denoting with \(\vb{y}_{i}\), \(\vb{a}_{i}\), \(\vb{b}_{i}\) the i-th row of \(Y\), column of \(A\) and
    \(B\) respectively, and, given a matrix \(S\), with \(s_{i, j}\) the element on row \(i\) and column \(j\),
    we shall rewrite this condition as a system of linear equations
    \begin{equation} \label{linear-system-x}
        \begin{aligned}
        & \left\{
            \begin{aligned}
                & x_{2, k} + \lambda x_{1, k} = d_{1, k} + \lambda f_{1, k} + \vb{y}_{1}\vb{a}_{k} +
                    \lambda \vb{y}_{1}\vb{b}_{k} \\
                & x_{3, k} + \lambda x_{2, k} = d_{2, k} + \lambda f_{2, k} + \vb{y}_{2}\vb{a}_{k} +
                    \lambda \vb{y}_{2}\vb{b}_{k} \\
                & \hdots \\
                & x_{\epsilon+1, k} + \lambda x_{\epsilon, k} =
                    d_{\epsilon, k} + \lambda f_{\epsilon, k} + \vb{y}_{\epsilon}\vb{a}_{k} +
                    \lambda \vb{y}_{\epsilon}\vb{b}_{k} \\
            \end{aligned}
        \right. \\
        & & (k = 1, ..., n - \epsilon - 1)
    \end{aligned}
    \end{equation}
    To approach this problem, we shall define the \(1 \times (\epsilon - 1)\) matrix \(W\) as the differences between
    subsequent rows in \(D\) and \(F\) flipping the sign every \(n - \epsilon - 1\) rows
    \begin{align*}
        W^T &=
        \begin{bmatrix}
            f_{2, k} - d_{1, k} \\
            - (f_{3, k} - d_{2, k}) \\
            \hdots \\
            f_{\epsilon, k} - d_{\epsilon - 1, k}
        \end{bmatrix}, &
        \text{with } (k &= 1, ..., n - \epsilon - 1).
    \end{align*}
    Calling \(M\) an \(M_{\epsilon - 2}\) matrix (as defined in \eqref{m-matrices}) built from \(A^*, B^*\) and
    solving for \(Z\) in
    \[
        ZM_{\epsilon - 2} = W
    \]
    yields a matrix of the form
    \[
        Z^T =
        \begin{bmatrix}
            \vb{y}_{1} - \vb{y}_{2} \\
            - (\vb{y}_{2} - \vb{y}_{3}) \\
            \hdots \\
            \vb{y}_{\epsilon-1} - \vb{y}_{\epsilon}
        \end{bmatrix}.
    \]
    We can now recover \(Y\) and, subsequently, \(X\) from the linear system in \eqref{linear-system-x} so that
    we may apply the transformation
    \[
        \begin{bmatrix}
            I^{(\epsilon)} & Y \\
            0 & I^{(m - \epsilon)}
        \end{bmatrix}
        \begin{bmatrix}
            L_{\epsilon} & D + \lambda F \\
            0 & A^* + \lambda B^*
        \end{bmatrix}
        \begin{bmatrix}
            I^{(\epsilon + 1)} & -X \\
            0 & I^{(n - \epsilon - 1)}
        \end{bmatrix}
        = \begin{bmatrix}
            L_{\epsilon} & 0 \\
            0 & A^* + \lambda B^*
        \end{bmatrix}.
    \]

    For completion, we shall lay out the algorithm for a single step of the procedure to compute
    Kronecker's canonical form \(K(\Gamma(\lambda))\) of a singular pencil of matrices in pseudocode.

    \pagebreak

    \begin{algorithm}
        \caption{Procedure to compute KCF of a singular pencil.}\label{alg:kcf-singular}
        \KwData{$\Gamma(\lambda) = A + \lambda B$: singular pencil}
        \KwResult{$K$: KCF of the pencil of matrices $\Gamma(\lambda)$}
        $V \gets $ polynomial of minimal degree $\in ker(\Gamma)$\;
        $Q \gets $ complete to a basis($V$)\;
        $P \gets $ complete to a basis($AV$)\;
        $\tilde{A} = P^{-1}AQ$\;
        $\tilde{B} = P^{-1}BQ$\;
        $L_{\epsilon}, D, F, A^*, B^* \gets $ partition pencil($\tilde{A} + \lambda \tilde{B}$)\;
        \eIf{$degree(V) \neq 0$}{
            $M \gets $ build $M_{\epsilon}(A^*, B^*, \epsilon-1)$\;
            $W \gets $ build $W(F, D)$\;
            $Z \gets WM^{-1}$\;
            $Y \gets $ partition(Z)\;
            $X \gets $ solve system in \eqref{linear-system-x}\;
            $L \gets $ block matrix($[[I, Y], [0, I]]$)\;
            $R \gets$ block matrix($[[I, -X], [0, I]]$)\;
            \Return{$LP^{-1}\Gamma(\lambda)QR$}\;
        }
        {\Return{$P^{-1}\Gamma(\lambda)Q$}\;}
    \end{algorithm}

    We have now proved the following theorem.
    \begin{theorem}[Reduction theorem]
        Given a pencil of matrices \(\Gamma(\lambda) = A + \lambda B\), if the polynomial of minimal degree \(\epsilon\)
        in the (right) kernel of \(\Gamma(\lambda)\) has degree \(\epsilon > 0\), then \(\Gamma(\lambda)\) is equivalent
        to a pencil of the form
        \[
            \begin{bmatrix}
                L_{\epsilon} & 0 \\
                0 & A^* + \lambda B^*
            \end{bmatrix}
        \]
    \end{theorem}
    \case \(\epsilon = 0.\)

    In this case, the columns of the pencil are connected by linear relations with constant coefficients.

    This implies the transformation given by matrices \(P, Q\) returns a pencil of the form
    \[
        \begin{bmatrix}
            0 & A^* + \lambda B^*
        \end{bmatrix},
    \]
    and this procedure can be conducted as many times as the number of independent constant solutions of the
    equation \((A + \lambda B)x = 0\) so that we may reduce the problem to \textbf{Case 1}.
\end{cs}