\section{Background}
This chapter will serve as prerequisite knowledge throughout the rest of this thesis.

We shall briefly present SageMath, the software system used to implement the algorithm discussed in the following chapters,
by introducing computer algebra systems and comparing numerical computations against computer algebra showcasing an
example; then, the reader shall familiarize with the concept of condition number as an emphasis on it will be put in
subsequent sections.

Next, definitions and properties of eigenvalues and eigenvectors shall be concisely introduced.

Lastly, we shall describe Jordan's canonical form of a matrix.

\subsection*{Computer algebra.}
Computers have fundamentally two ways to reason about a mathematical expression: \textbf{numerical computations}, which are
performed using \textit{only numbers} to represent values and \textbf{computer algebra} (or \textbf{symbolic computations}),
which - by contrast - use \textit{both numbers and symbols}.

First, we shall introduce the concept of \textbf{floating point number system}, which is the system used to handle numerical
computations.

For further explanations, refer to Quarteroni, Saleri, Sacco \cite{numerical-mathematics}.
\begin{definition}[Normalized-floating point number system] \cite[2.5.2]{numerical-mathematics}
    A normalized-floating point number system \(F\) is characterized by the 4-tuple of integers \(\beta, p, L, U\):
    \begin{itemize}[topsep=0pt, itemsep=0pt, parsep=0pt]
        \item \(\beta\) is called base or radix,
        \item \(p\) precision,
        \item \([L, U]\) exponent range (with \(L\), \(U\) denoting lower and upper bound respectively).
    \end{itemize}
    Given a number \(x \in \mathbb{R}\), \(x \neq 0\) its representation in a floating point number system shall be written
    out as \(fl(x)\) and has the form
    \[x = sign(x) \beta^E \sum_{i=0}^{p-1}d_{i}\beta^{-i}\]
    with \(L \leq E \leq U\) and the sequence \(\{d_{i}\}_{i \geq 0}\) (which is called mantissa) made up of
    natural numbers such that \(d_{0} \neq 0\), \(0 \leq d_{i} \leq \beta - 1\) and \(d_{i}\)
    eventually different from \(\beta - 1\).

    The notation \(\delta x\) shall be used to denote the difference between a symbol \(x\) and its floating point
    approximation \(fl(x)\)
    \[
        \delta x = x - fl(x).
    \]
\end{definition}

It is important to notice that a floating point number system \(F\) is discrete and finite: it approximates real numbers with
finite numbers; in other words, a floating point number system may introduce errors when representing a real number.

A de facto standard for computers to work with floating point approximations is IEEE 754 \cite{ieeefp}, the details of which
shall not be discussed.

\begin{definition}[Machine epsilon] \cite[p. 49]{numerical-mathematics}
    Machine epsilon is the maximum possible absolute relative error in representing a nonzero real number \(x\) in a
    floating point number system
    \[\epsilon_{mach} = \max \limits_{x} \dfrac{\vert x - fl(x) \vert}{\vert x \vert}.\]
\end{definition}

\begin{example}
    Let us define a matrix $M$ made up of both symbols and numbers,
    \[
        M = \begin{bmatrix}
            \sqrt{2}  &   1    \\
                2     & \sqrt{2}
        \end{bmatrix}.
    \]
    Consider the matrix \( \tilde{M} \), having as entries the floating point approximation of those of M
    \[
        \begin{bmatrix}
            fl(\sqrt{2})      &         1      \\
                2             &    fl(\sqrt{2})
        \end{bmatrix}.
    \]
    Computing its determinant gives out \( 2  +2\epsilon\sqrt{2} + \epsilon^2 - 2 \doteq 2 + 2\epsilon\sqrt{2} -2 \neq 0 \).
\end{example}

Introducing a small change (i.e. an ``error") in the input argument may either cause a large or a small change in the result.
Now, we shall define what condition numbers are.

\begin{definition}[Condition number]\cite[p. 33]{numerical-mathematics}
    A condition number of a problem measures the sensitivity of the solution to small perturbations in the input data.
    Given a function \(f \), we define
    \[
        cond(f, x) = \lim_{\epsilon \to 0} \sup \limits_{\norm{\Delta x} \leq \epsilon \norm{x}}
        \dfrac{\norm{f(x+\Delta x) - f(x)}}{\epsilon \norm{f(x)}}.
    \]
    Given a problem, if its condition number is low it is said to be \textbf{well-conditioned}
    (typically \( cond(f, x) \sim 1 \)), while a problem with a high condition number is (said to be)
    \textbf{ill-conditioned} (\( cond(f, x) \gg 1 \)).
\end{definition}

Let us now consider the problem of solving a linear equation subjected to a perturbation.

Let A be a non-singular matrix and assume we introduce a perturbation in the constant term
\(\tilde{\vb{b}} = \vb{b} +  \delta \vb{b}\). The equation can be written as
\[
    A\tilde{\vb{x}} = \tilde{\vb{b}}
\]
with \(\tilde{\vb{x}} = \vb{x} + \delta \vb{x}\). We can obtain
\[
    \tilde{\vb{x}} - \vb{x} = A^{-1} \tilde{\vb{b}} - A^{-1} \vb{b} = A^{-1} \delta \vb{b}
\]
and, by using matrix norms, we can write
\[
    \norm{\tilde{\vb{x}} - \vb{x}} = \norm{A^{-1} \delta \vb{b}} \leq \norm{A^{-1}} \norm{\delta \vb{b}}.
\]
It is also known that
\[
    \norm{\vb{b}} = \norm{A \vb{x}} \leq \norm{A} \norm{\vb{x}}
\]
which implies
\[
    \norm{\vb{x}} \geq \dfrac{\norm{\vb{b}}}{\norm{A}}.
\]
Tying all this together we can conclude
\[
    \dfrac{\norm{\tilde{\vb{x}} - \vb{x}}}{\norm{\vb{x}}} \leq \norm{A} \norm{A^{-1}}
    \dfrac{\norm{\delta \vb{b}}}{\norm{\vb{b}}}.
\]

\begin{definition}[Condition number of a matrix]\cite[p. 36]{numerical-mathematics}
    The condition number of a non-singular matrix A is defined as
    \[
        \kappa(A) = \norm{A^{-1}} \norm{A}.
    \]
\end{definition}

Now, let us refocus on the topic of math expressions.
Let us investigate what would happen if symbols are allowed in computations by introducing a framework that enables
us to work with computer algebra.
\begin{definition}[Computer algebra system]
    A computer algebra system (CAS) is a mathematics software package that can perform \textit{both symbolic and numerical
    mathematical computations}.
\end{definition}

A CAS is usually a \textbf{REPL} expected to support a few functionalities \cite{introcas}:
\begin{itemize}[topsep=0pt, itemsep=0pt, parsep=0pt]
    \item \textbf{Arithmetic}:
        arithmetic over different fields with arbitrary precision.
    \item \textbf{Linear algebra}:
        matrix algebra and knowledge of different operations and properties of matrices
        (i.e. determinants, eigenvalues and eigenvectors).
    \item \textbf{Polynomial manipulation}:
        factorization over different fields, simplification and partial fraction decomposition of rational functions.
    \item \textbf{Transcendental functions}:
        support for transcendental functions and their properties.
    \item \textbf{Calculus}:
        limits, derivatives, integration and expansions of functions.
    \item \textbf{Solving equations}:
        solving systems of linear equations, computing with radicals solutions of polynomials of degree less than five.
    \item \textbf{Programming language}:
        users may implement their own algorithms using a programming language.
\end{itemize}

The CAS chosen for this work is \textbf{SageMath} \cite{sage}, the features and functionalities of which shall not
be discussed here.

SageMath is an open source CAS distributed under the terms of the GNU GPLv3 \cite{gpl}.

Hereafter, an example in which symbolic computations are put against numerical (computations) shall be made.
\begin{example}
    Take matrix $M$ from Example 2.1
    \[
        M = \begin{bmatrix}
            \sqrt{2}  &   1    \\
                2     & \sqrt{2}
        \end{bmatrix}.
    \]
    Compare the different results given out when computing its determinant by defining M over the \textit{symbolic ring SR} and
    the \textit{finite-precision ring CDF}
    \begin{minted}[mathescape,
            numbersep=5pt,
            gobble=2,
            frame=lines,
            framesep=2mm,
            escapeinside=||]{sage}
        |\textcolor{blue}{sage:}| matrix(SR, [[sqrt(2), 1], [2, sqrt(2)]]).det()
        0
        |\textcolor{blue}{sage:}| matrix(CDF, [[sqrt(2), 1], [2, sqrt(2)]]).det()
        -3.14018491736755e-16
    \end{minted}
    We can observe that in SR \((\sqrt{2})^2 = 2\) since no approximations are made.

    Now, take the polynomial \(p(x)\):
    \[
        p(x) = x^{6} + 5 \, x^{5} - 3 \, x^{4} - 42 \, x^{3} + 12 \, x^{2} - x + 1.
    \]
    If an attempt to calculate its roots over SR is made an exception will be thrown (here, a reader may refer to
    Abel-Ruffini theorem for further explanations); however, doing this over a finite-precision ring (such as CDF) will work:
    \begin{minted}[mathescape,
            numbersep=5pt,
            gobble=2,
            frame=lines,
            framesep=2mm,
            escapeinside=||]{sage}
        |\textcolor{blue}{sage:}| p = x^6 + 5*x^5 - 3*x^4 -42*x^3 + 12*x^2 - x + 1
        |\textcolor{blue}{sage:}| p.roots(ring=SR)
            RuntimeError: no explicit roots found
        |\textcolor{blue}{sage:}| p.roots(ring=CDF)
        [(-3.865705050148171 - 1.5654017866113432*I, 1),
        (-3.8657050501481702 + 1.5654017866113419*I, 1),
        (-0.04843174828928114 - 0.2430512799158686*I, 1),
        (-0.048431748289281144 + 0.24305127991586856*I, 1),
        (0.38275295887213723 + 7.286537374692244e-17*I, 1),
        (2.4455206380027437 - 1.995314986816126e-16*I, 1)]
    \end{minted}
\end{example}

What we may conclude from such an example is that numerical analysis is certainly a powerful tool as it allows for computations
which could not happen with computer algebra, \textbf{but} computer algebra being able to compute an exact answer without
any approximation will prove to be useful in our use case.

For deeper reasoning about the limits of computer algebra systems, one may refer to Mitic \cite{mitic}.

\subsection*{Eigenvalues, eigenvectors}
In the following section, we shall define \textbf{eigenvalues} and \textbf{eigenvectors} and discuss the numerical stability
of their computation; a reader may also consult Axler \cite{axler} or Strang \cite{strang09} for further explanations.

\begin{definition}[Eigenvalue, eigenvector]
    Given a linear transformation \(T\) in a finite-dimensional vector space \(V\) over a field \(F\) into itself and a nonzero
    vector
    \(\vb{v}\), \(\vb{v}\) is an eigenvector of \(T\) if and only if
    \[ A \vb{u} = \lambda \vb{u} \]
    with \(A\) the matrix representation of \(T\), \(\vb{u}\) the coordinate vector of \(\vb{u}\) and \(\lambda\) a scalar in
    \(F\) known as eigenvalue associated with \(\vb{v}\).

    Similarly, we can define a row vector \(\vb{x}_{L}\), and a scalar \(\lambda_{L}\) such that
    \[\vb{x}_{L}A = \lambda_{L}\vb{x}_{L},\]
    which are called \textbf{left eigenvector} and \textbf{left eigenvalue} respectively.
\end{definition}

\begin{remark}
    Note that writing
    \( A \vb{u} = \lambda \vb{u} \) is equivalent to \( (A - \lambda I)\vb{u} = 0 \).

    It follows that the eigenvalues of $A$ are the roots of
    \[
        det(A - \lambda I),
    \]
    which is a polynomial in \(\lambda\) known as the \textbf{characteristic polynomial} \(ch_{A}(\lambda)\).
\end{remark}

\begin{definition}[Eigenspace]
    Given a square matrix A and its eigenvalue \(\lambda\), we define the eigenspace of A associated with \(\lambda\) the subspace
    \(E_{A}\) of all vectors satisfying the equation
    \[E_{A} = \{\vb{u}: (A - \lambda I)\vb{u} = 0\} = ker(A - \lambda I).\]
\end{definition}

\begin{definition}[Algebraic, geometric multiplicities of eigenvalues]
    Given a square matrix \(A\) and a scalar \(\lambda \in \mathbb{C}\): we define the algebraic multiplicity of \(\lambda\) as
    \[ m_{A}(\lambda) = max\{k: (\exists s(x): s(x)(x - \lambda)^k = ch_{A}(x))\}.\]
    The geometric multiplicity of \(\lambda\) is defined as
    \[
        \nu_{A}(\lambda) = dim(ker(A - \lambda I)).
    \]
\end{definition}

\begin{remark}
    Suppose \(A\) is a real square matrix, then the following statements are true:
    \begin{itemize}[topsep=0pt, itemsep=0pt, parsep=0pt]
        \item the eigenvalues of the left and right eigenvectors of A are the same,
        \item the left eigenvectors simplify into the transpose of the right eigenvectors of \(A^T\).
    \end{itemize}
\end{remark}

Now, let us investigate how introducing perturbations in the representation of a matrix may influence the numerical stability of its
eigenvalues.

Let \(A\) be a square matrix, \(\lambda \in \mathbb{C}\) its eigenvalue, \(\vb{x}\), \(\vb{y}\) the right
and left eigenvectors associated with \(\lambda\). Consider the perturbed problem
\[
    \tilde{A} \tilde{\vb{x}} = \tilde{\lambda} \tilde{\vb{x}}
\]
with \(\epsilon\) the machine epsilon, \(\tilde{A} = A + \epsilon\delta A\), \(\tilde{\vb{x}} = \vb{x} +
\epsilon\delta \vb{x}\), \(\tilde{\lambda} = \lambda + \epsilon\delta \lambda\).

Differentiating w.r.t. \(\epsilon\) and multiplying by \(\vb{y}^T\) on the left side gives
\[
    \vb{y}^T \delta A \vb{x} + \vb{y}^T A fl(\vb{x}) = fl(\lambda) \vb{y}^T \vb{x} + \vb{y}^T \lambda fl(\vb{x})
\]
and, since \(\vb{y}\) is the left eigenvector we can rewrite it as
\[
    \dfrac{\delta \lambda}{\delta \epsilon} = \dfrac{\vb{y}^T \delta A \vb{x}}{\vb{y}^T \vb{x}}.
\]
Assuming \(\norm{\delta A} = 1\) and using the definition of dot product for \(\vb{y}^T \vb{x}\) we get
\[
    \vert \delta \lambda \vert \leq \dfrac{1}{\vert \cos(\theta_{\lambda}) \vert} \vert \delta \epsilon \vert.
\]

\begin{definition}[Condition number of an eigenvalue]\cite{stewart1990matrix}
    Given a square matrix \(A\), the eigenvalue \(\lambda \in \mathbb{C}\) and \(\theta_{\lambda}\) the angle between the left
    and right eigenvectors associated with \(\lambda\), the quantity
    \[k_{A}(\lambda) = \dfrac{1}{\cos(\theta_{\lambda})}\]
    is called the condition number of the eigenvalue \(\lambda\).
\end{definition}
    
\subsection*{Jordan's canonical form}
In the following section, we shall define \textbf{Jordan matrices} and discuss the stability of a transformation of a matrix
into Jordan's canonical form.
\begin{definition}[Jordan matrix]
    A diagonal block matrix $M$ is called a Jordan matrix if and only if each block along the diagonal is of the form
    \[
        \begin{bmatrix}
            \lambda     &    1         &    0     &   \cdots   &    0    \\
                0       &    \lambda   &    1     &   \cdots   &    0    \\
            \vdots      &    \vdots    &  \vdots  &   \ddots   & \vdots  \\
                0       &       0      &     0    &  \lambda   &    1    \\
                0       &       0      &     0    &     0      & \lambda
        \end{bmatrix}.
    \]

    Each \(n \times n\) block can be completely characterized by the tuple \((\lambda, n)\) as it can fully describe both
    the structure and the dimension of a block.
\end{definition}

\begin{remark}
    Let \(V\) be a vector space defined over a field \(F\) and \(A\) a matrix defined in V. If the characteristic
    polynomial of A \(ch_{A}(t)\) can be factorized into its linear factors over \(K\), then \(A\) is similar to a
    Jordan matrix \(J\). We define \(J\) \textbf{Jordan's canonical form} (\textbf{JCF}) of \(A\).
\end{remark}

\begin{definition}[Defective matrix, defective eigenvalue]
    Given a square \(n \times n\) matrix \(A\), if it has less than \(n\) distinct eigenvalues then it is called a
    defective matrix.

    Furthermore, we define an eigenvalue \(\lambda\) of such a matrix as a defective eigenvalue if and only if
    \[
        m_{A}(\lambda) > \nu_{A}(\lambda).
    \]
\end{definition}

Now, we shall give a result on the stability of such a transformation the proof of which can be found in other works, such as
Datta \cite{biswadatta}.

\begin{theorem}[Stability of the JCF transformation]\label{thm:stability-jcf}
    Given a matrix \(A\) and its JCF \(A = P^{-1}JP\), the transforming matrix P is highly ill-conditioned
    whenever A has at least a defective or nearly defective eigenvalue.
\end{theorem}

Lastly, we shall give an example to show the implications of this theorem by showing the differences in
the JCF of a matrix and its perturbed version.

\begin{example}
Consider the \(n \times n\) matrices
\[
    A =
    \begin{bmatrix}
            0       &      1       &    0     &   \cdots   &    0    \\
            0       &      0       &    1     &   \cdots   &    0    \\
        \vdots      &    \vdots    &  \vdots  &   \ddots   & \vdots  \\
            0       &       0      &     0    &     0      &    1    \\
            0       &       0      &     0    &     0      &    0
    \end{bmatrix},
    B =
    \begin{bmatrix}
        0       &      1       &    0     &   \cdots   &    0    \\
        0       &      0       &    1     &   \cdots   &    0    \\
     \vdots     &    \vdots    &  \vdots  &   \ddots   & \vdots  \\
        0       &       0      &     0    &     0      &    1    \\
      \alpha    &       0      &     0    &     0      &    0
    \end{bmatrix}
\]
with \(\alpha > 0\).

It is evident how \(A\) has a defective eigenvalue in \(\lambda_{A} = 0\) and \(m_{A}(0) = n\), \(\nu_{A}(0) = 1\); furthermore,
\(A\) is already in JCF.

Now, let us switch our focus to \(B\). To compute its eigenvalues, take the characteristic polynomial
\(ch_{B}(t) = t^n - \alpha\): it has n distinct roots in
\[
    t_{j} = z_{n}^{j} \sqrt[n]{\alpha}
\]
with \(j = 1, ..., n\), \(z_{n} = \cos(\dfrac{2\pi}{n}) + \iu \sin(\dfrac{2\pi}{n})\) and
\(\iu\) imaginary unit such that \(\iu^2 = -1\).

To conclude, we shall show the JCF of \(A\) and \(B\) defined in SR computed by SageMath when \(n = 4\).

\begin{minted}[mathescape,
    numbersep=5pt,
    gobble=2,
    frame=lines,
    framesep=2mm,
    escapeinside=!!]{sage}
    !\textcolor{blue}{sage:}! A = matrix(SR, [
                [1 if i == j-1
                    else 0 for j in range(4)]
                for i in range(4)
            ])
    !\textcolor{blue}{sage:}! B = matrix(SR, [
                [1 if i == j-1
                    else x if j == 0 and i == 3
                        else 0 for j in range(4)]
                for i in range(4)
            ])
    !\textcolor{blue}{sage:}! A
    [0 1 0 0]
    [0 0 1 0]
    [0 0 0 1]
    [0 0 0 0]
    !\textcolor{blue}{sage:}! A.jordan_form()
    [0 1 0 0]
    [0 0 1 0]
    [0 0 0 1]
    [0 0 0 0]
    !\textcolor{blue}{sage:}! B
    [0 1 0 0]
    [0 0 1 0]
    [0 0 0 1]
    [x 0 0 0]
    !\textcolor{blue}{sage:}! B.jordan_form()
    [ I*x^(1/4)|         0|         0|         0]
    [----------+----------+----------+----------]
    [         0|  -x^(1/4)|         0|         0]
    [----------+----------+----------+----------]
    [         0|         0|-I*x^(1/4)|         0]
    [----------+----------+----------+----------]
    [         0|         0|         0|   x^(1/4)]

\end{minted}

For the sake of clarity, we shall also show what the implications of B having such eigenvalues are.

Suppose \(x = 10^{-10}\).

\begin{minted}[mathescape,
    numbersep=5pt,
    gobble=2,
    frame=lines,
    framesep=2mm,
    escapeinside=||]{sage}
    |\textcolor{blue}{sage}|: B = matrix(SR, [
                [1 if i == j-1
                    else 10**-10 if j == 0 and i == 3
                        else 0 for j in range(4)]
                for i in range(4)
            ])
    |\textcolor{blue}{sage}|: B
    [            0             1             0             0]
    [            0             0             1             0]
    [            0             0             0             1]
    [1/10000000000             0             0             0]
    |\textcolor{blue}{sage}|: P =  = B.jordan_form(transformation=True)[1]
    |\textcolor{blue}{sage}|: cond = norm(P.inverse()) * norm(P)
    |\textcolor{blue}{sage}|: cond
    31622776.60168379
\end{minted}
We can see that \(\kappa(P) \gg 1\), as stated in \nameref{thm:stability-jcf} (theorem \ref{thm:stability-jcf}).

\end{example}