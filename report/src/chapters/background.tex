\section{Background}
\subsection*{Computer algebra.}
Computers have fundamentally two ways to reason over a mathematical expression: \textbf{numerical computations}, which are
performed using \textit{only numbers} to represent values and \textbf{computer algebra} (or \textbf{symbolic computations}),
which - by contrast - use \textit{both numbers and symbols}.

First, we shall introduce the concept of \textbf{floating point number system} used to handle numerical computations.
\begin{definition}[Normalized-floating point number system]
    A normalized-floating point number system \(F\) is characterized by the 4-tuple of integers \(\beta, p, L, U\):
    \begin{itemize}[topsep=0pt, itemsep=0pt, parsep=0pt]
        \item \(\beta\) is called base or radix,
        \item \(p\) precision,
        \item \([L, U]\) exponent range (with \(L\), \(U\) denoting lower and upper bound respectively).
    \end{itemize}
    Given a number \(x \in \mathbb{R}\), \(x \neq 0\) its representation in a floating point number system shall be written out as
    \(fl(x)\) and has the form
    \[x = sign(x) \beta^E \sum_{i=0}^{p-1}d_{i}\beta^{-i}\]
    with \(L \leq E \leq U\) and the sequence \(\{d_{i}\}\) (which is called mantissa) made up of natural numbers such that
    \(d_{0} \neq 0\), \(0 \leq d_{i} \leq \beta - 1\) and \(d_{i}\) eventually different from \(\beta - 1\).
\end{definition}
\begin{remark}
    A floating point number system \(F\) is discrete and finite: it approximates real numbers with finite numbers.
\end{remark}

A de facto standard for computers to work with floating point approximations is IEEE 754 \cite{ieeefp}, the details of which
shall not be discussed.

\begin{definition}[Machine epsilon]
    Machine epsilon is the maximum possible absolute relative error in representing a nonzero real number \(x\) in a floating point
    number system
    \[\epsilon_{mach} = \max \limits_{x} \dfrac{\vert x - fl(x) \vert}{\vert x \vert}.\]
\end{definition}

\begin{example}
    Let us define the matrix (made up of both symbols and numbers) M
    \[
        \begin{bmatrix}
            \sqrt{2}  &   1    \\
                2     & \sqrt{2}
        \end{bmatrix}.
    \]
    Consider the matrix \( \tilde{M} \), having as entries the floating point approximation of those of M
    \[
        \begin{bmatrix}
            fl(\sqrt{2})      &         1      \\
                2             &    fl(\sqrt{2})
        \end{bmatrix}.
    \]
    Computing its determinant gives out \( 2  +2\epsilon\sqrt{2} + \epsilon^2 - 2 \doteq 2 + 2\epsilon\sqrt{2} -2 \neq 0 \).
\end{example}

Introducing a small change (i.e. an ``error") in the input argument may either cause a large or a small change in the result.
We shall now introduce the concept of condition numbers.

\begin{definition}[Condition number]
    A condition number of a problem measures the sensitivity of the solution to small perturbations in the input data.
    Given a function \(f \), we define
    \[
        cond(f, x) = \lim_{\epsilon \to 0} \sup \limits_{\norm{\Delta x} \leq \epsilon \norm{x}}
        \dfrac{\norm{f(x+\Delta x) - f(x)}}{\epsilon \norm{f(x)}}.
    \]
    Given a problem, if its condition number is low it is said to be \textbf{well-conditioned} (typically \( cond(f, x) \sim 1 \)),
    while a problem with a high condition number is (said to be) \textbf{ill-conditioned} (\( cond(f, x) \gg 1 \)).
\end{definition}

\begin{definition}[Condition number of a matrix]
    The condition number of a non-singular matrix A is defined as:
    \[
        \kappa(A) = \norm{A^{-1}} \times \norm{A}.
    \]
\end{definition}

Let us now investigate what would happen if symbols are allowed by introducing a framework that allows us to work both
with numerical and symbolic computations.
\begin{definition}[Computer algebra system]
    A computer algebra system (CAS) is a mathematics software package that can perform \textit{both symbolic and numerical
    mathematical computations}.
\end{definition}

A CAS is usually a \textbf{REPL} expected to support a few functionalities \cite{introcas}:
\begin{itemize}[topsep=0pt, itemsep=0pt, parsep=0pt]
    \item \textbf{Arithmetic}:
        arithmetic over different fields with arbitrary precision.
    \item \textbf{Linear algebra}:
        matrix algebra and knowledge of different operations and properties of matrices
        (i.e. determinants, eigenvalues and eigenvectors).
    \item \textbf{Polynomial manipulation}:
        factorization over different fields, simplification and partial fraction decomposition of rational functions.
    \item \textbf{Transcendental functions}:
        support for transcendental functions and their properties.
    \item \textbf{Calculus}:
        limits, derivatives, integration and expansions of functions.
    \item \textbf{Solving equations}:
        solving systems of linear equations, computing with radicals solutions of polynomials of degree less than five.
    \item \textbf{Programming language}:
        users may implement their own algorithms using a programming language.
\end{itemize}

The CAS chosen for this work is \textbf{SageMath} \cite{sage}, the features and functionalities of which shall not
be discussed here.

SageMath is an open source CAS distributed under the terms of the GNU GPLv3 \cite{gpl}.

Hereafter, an example in which symbolic computations are put against numerical (computations) shall be made.
\begin{example}
    Take matrix M from Example 2.1:
    \[
        \begin{bmatrix}
            \sqrt{2}  &   1    \\
                2     & \sqrt{2}
        \end{bmatrix}.
    \]
    Compare the different results given out when computing its determinant by defining M over the \textit{symbolic ring SR} and
    the \textit{finite-precision ring CDF}:
    \begin{minted}[mathescape,
            numbersep=5pt,
            gobble=2,
            frame=lines,
            framesep=2mm]{sage}
        sage: matrix(SR, [[sqrt(2), 1], [2, sqrt(2)]]).det()
        0
        sage: matrix(CDF, [[sqrt(2), 1], [2, sqrt(2)]]).det()
        -3.14018491736755e-16
    \end{minted}
    We can observe that in SR \((\sqrt{2})^2 = 2\) since no approximations are made.

    Now, take the polynomial \(p(x)\):
    \[
        p(x) = x^{6} + 5 \, x^{5} - 3 \, x^{4} - 42 \, x^{3} + 12 \, x^{2} - x + 1.
    \]
    If an attempt to calculate its roots over SR is made, an exception will be thrown; however, doing this over a
    finite-precision ring (such as CDF) will work:
    \begin{minted}[mathescape,
            numbersep=5pt,
            gobble=2,
            frame=lines,
            framesep=2mm]{sage}
        sage: p = x^6 + 5*x^5 - 3*x^4 -42*x^3 + 12*x^2 - x + 1
        sage: p.roots(ring=SR)
            RuntimeError: no explicit roots found
        sage: p.roots(ring=CDF)
        [(-3.865705050148171 - 1.5654017866113432*I, 1),
        (-3.8657050501481702 + 1.5654017866113419*I, 1),
        (-0.04843174828928114 - 0.2430512799158686*I, 1),
        (-0.048431748289281144 + 0.24305127991586856*I, 1),
        (0.38275295887213723 + 7.286537374692244e-17*I, 1),
        (2.4455206380027437 - 1.995314986816126e-16*I, 1)]
    \end{minted}
\end{example}

For deeper reasoning about the limits of computer algebra systems, one may refer to Mitic \cite{mitic}.

\subsection*{Eigenvalues, eigenvectors}
In the following section, eigenvalues and eigenvectors shall be defined.

Lastly, a result on the condition number of the problem
of computing eigenvalues of a matrix shall be given.
\begin{definition}[Eigenvalue, eigenvector]
    Given a linear transformation \(T\) in a finite-dimensional vector space \(V\) over a field \(F\) into itself and a nonzero
    vector
    \(\vb{v}\), \(\vb{v}\) is an eigenvector of \(T\) if and only if
    \[ A \vb{u} = \lambda \vb{u} \]
    with \(A\) the matrix representation of \(T\), \(\vb{u}\) the coordinate vector of \(\vb{u}\) and \(\lambda\) a scalar in
    \(F\) known as eigenvalue associated with \(\vb{v}\).

    Similarly, we can define a row vector \(\vb{x}_{L}\), and a scalar \(\lambda_{L}\) such that
    \[\vb{x}_{L}A = \lambda_{L}A,\]
    which are called left eigenvector and left eigenvalue respectively.
\end{definition}

\begin{remark}
    Note that writing
    \( A \vb{u} = \lambda \vb{u} \) is equivalent to \( (A - \lambda I)\vb{u} = 0 \).

    It follows that the eigenvalues of A are the roots of
    \[ det(A - \lambda I) \]
    which is a polynomial in \(\lambda\) known as the \textbf{characteristic polynomial} \(ch(A)\).
\end{remark}

\begin{definition}[Eigenspace]
    Given a square matrix A and its eigenvalue \(\lambda\), we define the eigenspace of A associated with \(\lambda\) the subspace
    \(E_{A}\) of all vectors satisfying the equation
    \[E_{A} = \{\vb{u}: (A - \lambda I)\vb{u} = 0\} = ker(A - \lambda I).\]
\end{definition}

\begin{remark}
    Suppose \(A\) is a real square matrix, then the following statements are true:
    \begin{itemize}[topsep=0pt, itemsep=0pt, parsep=0pt]
        \item the eigenvalues of the left and right eigenvectors of A are the same,
        \item the left eigenvectors simplify into the transpose of the right eigenvectors of \(A^T\).
    \end{itemize}
\end{remark}

\begin{definition}[Algebraic, geometric multiplicities of eigenvalues]
    Given a square matrix \(A\) and a scalar \(\lambda \in \mathbb{C}\): we define the algebraic multiplicity of \(\lambda\) as
    \[ m_{A}(\lambda) = max\{k: (\exists s(x): s(x)(x - \lambda)^k = ch_{A}(x))\}.\]
    The geometric multiplicity of \(\lambda\) is defined as
    \[
        \nu_{A}(\lambda) = dim(ker(A - \lambda I)).
    \]
\end{definition}

Let us now investigate how introducing perturbations in the representation of a matrix may influence the numerical stability of its
eigenvalues (caveat: in the following paragraph, the notation \(\delta x\) shall be used to denote the difference between a symbol
\(x\) and its floating point approximation \(fl(x)\)).

Let us define a square matrix \(A\) and its eigenvalue \(\lambda \in \mathbb{C}\), \(\vb{x}\), \(\vb{y}\) the right
and left eigenvectors associated with \(\lambda\).

Consider the perturbed problem
\[
    \tilde{A} \tilde{\vb{x}} = \tilde{\lambda} \tilde{\vb{x}}
\]
with \(\epsilon\) the machine epsilon, \(\tilde{A} = A + \epsilon\delta A\), \(\tilde{\vb{x}} = \vb{x} +
\epsilon\delta \vb{x}\), \(\tilde{\lambda} = \lambda + \epsilon\delta \lambda\).

Differentiating w.r.t. \(\epsilon\) and multiplying by \(\vb{y}^T\) on the left side gives
\[
    \vb{y}^T \delta A \vb{x} + \vb{y}^T A fl(\vb{x}) = fl(\lambda) \vb{y}^T \vb{x} + \vb{y}^T \lambda fl(\vb{x})
\]
and, since \(\vb{y}\) is the left eigenvector we can rewrite it as
\[
    \dfrac{\delta \lambda}{\delta \epsilon} = \dfrac{\vb{y}^T \delta A \vb{x}}{\vb{y}^T \vb{x}}.
\]
Assuming the absolute error \(\norm{\delta A} = 1\) and using the definition of cross product for \(\vb{y}^T \vb{x}\) we get
\[
    \vert \delta \lambda \vert \leq \dfrac{1}{\vert cos(\theta_{\lambda}) \vert} \vert \delta \epsilon \vert.
\]

\begin{definition}[Condition number of an eigenvalue]
    Given a square matrix \(A\), the eigenvalue \(\lambda \in \mathbb{C}\) and \(\theta_{\lambda}\) the angle between the left
    and right eigenvectors associated with \(\lambda\), the quantity
    \[k_{A}(\lambda) = \dfrac{1}{cos(\theta_{\lambda})}\]
    is called the condition number of the eigenvalue \(\lambda\).
\end{definition}
    
\subsection*{Jordan canonical form}
\begin{definition}[Generalized eigenvector, generalized eigenspace]
    Given an \(n \times n\) square matrix \(A\) and its eigenvalue \(\lambda\), the vector \(\vb{x}_{m}\) such that
    \[
        (A - \lambda I)^{m} \vb{x}_{m} = 0 \ \wedge \ (A - \lambda I)^{m} \vb{x}_{m} \neq 0
    \]
    is a generalized eigenvector of rank \(m\) associated with \(\lambda\).

    Furthermore, the subspace \(E_{A}^m\)
    \[
        E_{A}^m(\lambda) = ker((A - \lambda I)^m)
    \]
    is called generalized eigenspace;
    its dimension
    \[
        \nu_{A}^{m}(\lambda) = dim(E_{A}^{m}(\lambda)) = n - rank((A - \lambda I)^m)
    \]
    is called the m-th geometric multiplicity of \(\lambda\) in \(A\).
\end{definition}

\begin{definition}[Defective matrix, defective eigenvalue]
    Given a square \(n \times n\) matrix \(A\), if it has less than \(n\) distinct eigenvalues then it is called a
    defective matrix.

    Furthermore, we define an eigenvalue \(\lambda\) of such a matrix as a defective eigenvalue if and only if
    \[
        m_{A}(\lambda) > \nu_{A}(\lambda).
    \]
\end{definition}

\begin{example}
    Consider the matrix M
    \[
        \begin{bmatrix}
            2 & 1 \\
            0 & 2
        \end{bmatrix},
    \]
    there is only one eigenvalue \(\lambda\) associated with M and \(\lambda = 2\); the eigenvector associated with
    it is
    \[
        \vb{x}_{1} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}.
    \]

    Furthermore, note that \(\lambda\) has algebraic multiplicity \(m_{A}(2) = 2\): it follows that M is a defective matrix
    and \(\lambda\) is a defective eigenvalue.

    Now, we shall compute its generalized eigenvectors.

    Note that \(dim(ker(A - \lambda I)) = p = 1\), which implies
    there exist \(m - p = 1\) generalized eigenvectors of rank greater than \(1\). To compute the generalized eigenvector
    \(\vb{x}_{2}\) we solve \((A - \lambda I)\vb{x}_{2} = \vb{x}_{1}\)
    \[
        M - \lambda
        \begin{bmatrix}
            1 & 0 \\
            0 & 1
        \end{bmatrix}
        \begin{bmatrix}
            x_{20} \\
            x_{21}
        \end{bmatrix}
        =
        \begin{bmatrix}
            1 \\
            0
        \end{bmatrix}.
    \]
    Substituting with their values gives us
    \[
        \begin{bmatrix}
            2 - 2*1 & 1       \\
                0   & 2 - 2*1
        \end{bmatrix}
        \begin{bmatrix}
            x_{20} \\
            x_{21}
        \end{bmatrix}
        =
        \begin{bmatrix}
            1 \\
            0
        \end{bmatrix}.
    \]
    Solving this system we can conclude
    \[
        \vb{x}_{2} = \begin{bmatrix} t \\ 1 \end{bmatrix}
    \]
    with no restrictions over the value of the scalar \(t\).
\end{example}

\begin{definition}[Jordan matrix]
    A diagonal block matrix M is called a Jordan matrix if and only if each block along the diagonal is of the form
    \[
        \begin{bmatrix}
            \lambda     &    1         &    0     &   \cdots   &    0    \\
                0       &    \lambda   &    1     &   \cdots   &    0    \\
            \vdots      &    \vdots    &  \vdots  &   \ddots   & \vdots  \\
                0       &       0      &     0    &  \lambda   &    1    \\
                0       &       0      &     0    &     1      & \lambda
        \end{bmatrix},
    \]
    and we indicate such a matrix with
    \(diag(J_{\lambda_{1}, n_{1}}, ..., J_{\lambda_{k}, n_{k}})\) with \(k\) the number of diagonal blocks it is made up of.

    Each block can be completely described by the tuple \((\lambda, n)\) as it is an \(n \times n\) matrix of zeroes everywhere
    except for the diagonal, which is filled with \(\lambda\), and the superdiagonal, with ones.
\end{definition}

\begin{theorem}[Jordan canonical form]
    
\end{theorem}

\begin{theorem}[Stability of the JCF transformation]
    Given a matrix \(A\) and its JCF \(A = P^{-1}JP\), the transforming matrix P is highly ill-conditioned
    whenever A has at least a defective or nearly defective eigenvalue.
\end{theorem}
\begin{proof}
    Work in progress...
\end{proof}